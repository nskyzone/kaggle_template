{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 处理数据\n",
    "\n",
    "NLP问题,去停用词，分词，词向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas  as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import jieba\n",
    "import jieba.posseg\n",
    "import jieba.analyse\n",
    "import codecs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 分词 停用词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分词\n",
    "def split_word(query, stopwords):\n",
    "    wordList = jieba.cut(query)\n",
    "    num = 0\n",
    "    result = ''\n",
    "    for word in wordList:\n",
    "        word = word.rstrip()\n",
    "        word = word.rstrip('\"')\n",
    "        if word not in stopwords:\n",
    "            if num == 0:\n",
    "                result = word\n",
    "                num = 1\n",
    "            else:\n",
    "                result = result + ' ' + word\n",
    "    return result.encode('utf-8')\n",
    "\n",
    "# 处理停用词 Series\n",
    "def deal_stop_word(data):\n",
    "    stopwords = {}\n",
    "    for line in codecs.open('../data/stop.txt','r','gbk'):\n",
    "        stopwords[line.rstrip()]=1\n",
    "    doc = data.map(lambda x:split_word(x,stopwords))\n",
    "    return doc\n",
    "\n",
    "def preprocess(data):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "def get_tfidf(train,predict):\n",
    "    vector = TfidfVectorizer(max_features=10000)\n",
    "    train_words_len = train.words.shape[0]\n",
    "    tfidf = vector.fit_transform(train.words.append(predict.words))\n",
    "    tfidf_dt = tfidf.toarray()\n",
    "    x_train = tfidf_dt[:train_words_len,:]\n",
    "    x_predit = tfidf_dt[train_words_len:,:]\n",
    "    return x_train,x_predict\n",
    "\n",
    "def save_tfidf(train_tfidf, predict_tfidf):\n",
    "    np.save('../data/train_tfidf',train_tfidf)\n",
    "    np.save('../data/predict_tfidf',predict_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "def get_embedding(docs,model_path='embedding'):\n",
    "    model = Word2Vec(docs.values.tolist(),\n",
    "                     size=100,  # 词向量维度\n",
    "                     min_count=5,  # 词频阈值\n",
    "                     window=5)  # 窗口大小\n",
    "    model.save('../data/' + model_path + '.mldel')\n",
    "\n",
    "\n",
    "# 构建embedding_matrix    \n",
    "def get_embedding_matrix(model_path,tok_raw,save=False):\n",
    "    model = Word2Vec.load(model_path)\n",
    "    word_index = tok_raw.word_index\n",
    "    embedding_matrix = np.zeros((len(word_index) + 1, model.vector_size))\n",
    "\n",
    "    for word, i in word_index.items():\n",
    "        if word in model.wv.vocab:\n",
    "            vector = model.wv[word]\n",
    "            if vector is not None:\n",
    "                embedding_matrix[i] = vector\n",
    "    if save:\n",
    "        np.save('embedding_matrix',embedding_matrix)\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 加载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Discuss</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>201e8bf2-77a2-3a98-9fcf-4ce03914e712</td>\n",
       "      <td>好大的一个游乐公园，已经去了2次，但感觉还没有玩够似的！会有第三，第四次的</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>f4d51947-eac4-3005-9d3c-2f32d6068a2d</td>\n",
       "      <td>新中国成立也是在这举行，对我们中国人来说有些重要及深刻的意义！</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>74aa7ae4-03a4-394c-bee0-5702d3a3082a</td>\n",
       "      <td>庐山瀑布非常有名，也有非常多个瀑布，只是最好看的非三叠泉莫属，推荐一去</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>099661c2-4360-3c49-a2fe-8c783764f7db</td>\n",
       "      <td>个人觉得颐和园是北京最值的一起的地方，不过相比下门票也是最贵的，比起故宫的雄伟与气势磅礴，颐...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>97ca672d-e558-3542-ba7b-ee719bba1bab</td>\n",
       "      <td>迪斯尼一日游</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Id  \\\n",
       "0  201e8bf2-77a2-3a98-9fcf-4ce03914e712   \n",
       "1  f4d51947-eac4-3005-9d3c-2f32d6068a2d   \n",
       "2  74aa7ae4-03a4-394c-bee0-5702d3a3082a   \n",
       "3  099661c2-4360-3c49-a2fe-8c783764f7db   \n",
       "4  97ca672d-e558-3542-ba7b-ee719bba1bab   \n",
       "\n",
       "                                             Discuss  Score  \n",
       "0              好大的一个游乐公园，已经去了2次，但感觉还没有玩够似的！会有第三，第四次的      5  \n",
       "1                    新中国成立也是在这举行，对我们中国人来说有些重要及深刻的意义！      4  \n",
       "2                庐山瀑布非常有名，也有非常多个瀑布，只是最好看的非三叠泉莫属，推荐一去      4  \n",
       "3  个人觉得颐和园是北京最值的一起的地方，不过相比下门票也是最贵的，比起故宫的雄伟与气势磅礴，颐...      5  \n",
       "4                                             迪斯尼一日游      5  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.read_csv('../data/train_first.csv')\n",
    "predict_data = pd.read_csv('../data/predict_first.csv')\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['doc'] = deal_stop_word(train_data['Discuss'])\n",
    "predict_data['doc'] = deal_stop_word(predict_data['Discuss'])\n",
    "# 整合数据\n",
    "docs = np.hstack([train_data.doc.values,predict_data.doc.values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_train = pd.concat((a_train1,a_train2,train_3 ,train_4,train_5),ignore_index=True)\n",
    "bb=b_train.drop(columns=['Id'])\n",
    "bb.to_csv('../data/clean_train2.csv',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

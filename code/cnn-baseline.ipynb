{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import pandas  as pd\n",
    "import numpy as np\n",
    "from  keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import  pad_sequences\n",
    "from keras.layers import Input, Dropout, Dense, BatchNormalization, Activation, concatenate, GlobalMaxPool1D,GRU, Embedding,Bidirectional, Flatten,LSTM, BatchNormalization,Conv1D,MaxPooling1D\n",
    "from keras.models import Model\n",
    "from keras.layers import GlobalMaxPooling1D\n",
    "from keras.layers import *\n",
    "from keras.layers.convolutional import Convolution1D\n",
    "from keras import optimizers\n",
    "from keras.callbacks import ModelCheckpoint, Callback, EarlyStopping\n",
    "from keras import backend as K\n",
    "from keras import regularizers\n",
    "import re\n",
    "import jieba\n",
    "import jieba.posseg\n",
    "import jieba.analyse\n",
    "import codecs\n",
    "from keras.layers import Input, Concatenate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('../data/train_first.csv')\n",
    "test = pd.read_csv('../data/predict_first.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 全局变量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_features = 80000 ## 词汇量  \n",
    "maxlen = 150  ## 最大长度\n",
    "embed_size = 200 # emb 长度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 预处理：缺失值填充+结巴分词+去停用词\n",
    "###  将词映射为ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def splitWord(query, stopwords):\n",
    "    wordList = jieba.cut(query)\n",
    "    num = 0\n",
    "    result = ''\n",
    "    for word in wordList:\n",
    "        word = word.rstrip()\n",
    "        word = word.rstrip('\"')\n",
    "        if word not in stopwords:\n",
    "            if num == 0:\n",
    "                result = word\n",
    "                num = 1\n",
    "            else:\n",
    "                result = result + ' ' + word\n",
    "    return result.encode('utf-8')\n",
    "def preprocess(data):\n",
    "    stopwords = {}\n",
    "    for line in codecs.open('../data/stop.txt','r','utf-8'):\n",
    "        stopwords[line.rstrip()]=1    \n",
    "    data['doc'] = data['Discuss'].map(lambda x:splitWord(x,stopwords))\n",
    "    return data;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train.Discuss.fillna('_na_',inplace=True)\n",
    "test.Discuss.fillna('_na_',inplace=True)\n",
    "train = preprocess(train)\n",
    "test = preprocess(test)\n",
    "\n",
    "comment_text = np.hstack([train.doc.values])\n",
    "tok_raw = Tokenizer(num_words=max_features)\n",
    "tok_raw.fit_on_texts(comment_text)\n",
    "train['Discuss_seq'] = tok_raw.texts_to_sequences(train.doc.values)\n",
    "test['Discuss_seq'] = tok_raw.texts_to_sequences(test.doc.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_keras_data(dataset): \n",
    "    X={\n",
    "        'Discuss_seq':pad_sequences(dataset.Discuss_seq,maxlen=maxlen)\n",
    "    }\n",
    "    return X\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### text-cnn 多filter_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def score(y_true, y_pred):\n",
    "    return 1.0/(1.0+K.sqrt(K.mean(K.square(y_true - y_pred), axis=-1)))\n",
    "\n",
    "def cnn():\n",
    "    #Inputs\n",
    "    comment_seq = Input(shape=[maxlen],name='Discuss_seq')\n",
    "    \n",
    "    #Embeddings layers\n",
    "    emb_comment =Embedding(max_features, embed_size)(comment_seq)\n",
    "    \n",
    "    # conv layers\n",
    "    convs = []\n",
    "    filter_sizes = [2,3,4,5]\n",
    "    for fsz in filter_sizes:\n",
    "        l_conv = Conv1D(filters=100,kernel_size=fsz,activation='relu')(emb_comment)\n",
    "        l_pool = MaxPooling1D(maxlen-fsz+1)(l_conv)\n",
    "        l_pool = Flatten()(l_pool)\n",
    "        convs.append(l_pool)\n",
    "    merge =concatenate(convs,axis=1)\n",
    "    \n",
    "    out = Dropout(0.5)(merge)\n",
    "    output  = Dense(32,activation='relu')(out)\n",
    "    \n",
    "    output = Dense(units=1,activation='linear')(output)\n",
    "    \n",
    "    model = Model([comment_seq],output)\n",
    "    adam = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "    model.compile(loss=\"mse\", optimizer=\"adam\", metrics=[\"mae\", score])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### rnn 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rnn1():\n",
    "    comment_seq = Input(shape=[maxlen],name='Discuss_seq')\n",
    "    \n",
    "    #Embeddings layers\n",
    "    emb_comment =Embedding(max_features, embed_size, weights=[embedding_matrix])(comment_seq)\n",
    "    main = Bidirectional(GRU(50, return_sequences=True, dropout=0.2, recurrent_dropout=0.4))(emb_comment)\n",
    "#     main1 = Bidirectional(LSTM(32, return_sequences=True, dropout=0.1, recurrent_dropout=0.2))(emb_comment)\n",
    "    main = GlobalMaxPool1D()(main)\n",
    "#     main1 = GlobalMaxPool1D()(main1)\n",
    "#     main =concatenate([main,main1],axis=1)\n",
    "    main = Dense(50, activation=\"relu\")(main)\n",
    "    main= Dropout(0.2)(main)\n",
    "    main = Dense(units=1,activation='linear')(main)\n",
    "    model = Model([comment_seq],main)\n",
    "    adam = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "    model.compile(loss=\"mse\", optimizer=\"adam\", metrics=[\"mae\", score])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cnn_rnn 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def cnn_rnn():\n",
    "    comment_seq = Input(shape=[maxlen],name='Discuss_single_seq')\n",
    "    \n",
    "    #Embeddings layers\n",
    "    main =Embedding(max_features, embed_size)(comment_seq)\n",
    "    main = Conv1D(filters=32, kernel_size=2, padding='same', activation='relu')(main)\n",
    "    main = MaxPooling1D(pool_size=2)(main)\n",
    "    main = Conv1D(filters=32, kernel_size=2, padding='same', activation='relu')(main)\n",
    "    main = MaxPooling1D(pool_size=2)(main)\n",
    "    main = Bidirectional(GRU(32))(main)\n",
    "    main = Dense(units=1,activation='linear')(main)\n",
    "    \n",
    "    model = Model([comment_seq],main)\n",
    "    adam = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "    model.compile(loss=\"mse\", optimizer=\"adam\", metrics=[\"mae\", score])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train =get_keras_data(train)\n",
    "X_test = get_keras_data(test)\n",
    "y_train = train.Score.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128 \n",
    "epochs = 20\n",
    "early_stopping = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=2)\n",
    "\n",
    "callbacks_list = [early_stopping]\n",
    "model = cnn()\n",
    "model.summary()\n",
    "model.fit(X_train, y_train,\n",
    "            validation_split=0.1,\n",
    "            batch_size=batch_size, \n",
    "            epochs=epochs, \n",
    "            shuffle = True,\n",
    "            callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds = model.predict(X_test)\n",
    "submission =pd.DataFrame(test.Id.values,columns=['Id'])\n",
    "submission['Score'] = preds\n",
    "submission.to_csv('../result/cnn-baseline.csv',index=None,header =None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
